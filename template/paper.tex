%!TEX program = xelatex
%!TEX TS-program = xelatex
%!TEX encoding = UTF-8 Unicode

\documentclass[algorithmlist, AutoFakeBold, AutoFakeSlant, figurelist, tablelist, nomlist, masters]{seuthesix}
\usepackage{xeCJK}
\usepackage{fontspec, xltxtra, xunicode}
\usepackage{graphicx, subfig}
\usepackage{autobreak}
\usepackage{amsmath, amssymb}
\usepackage{tabularx, array, multirow}
\usepackage{float}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{enumerate}
\usepackage{longtable}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{natbib}

\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}} %UseOutput in the format of Algorithm


\XeTeXlinebreaklocale “zh” 
\XeTeXlinebreakskip = 0pt plus 1pt minus 0.1pt %文章内中文自动换行
%公式编号设置
% \numberwithin{equation}{section}
% % \makeatletter
% % \@addtoreset{equation}{section}
% % \makeatother
\renewcommand\theequation{\arabic{chapter}-\arabic{equation}}

\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
    \begin{center}
      \refstepcounter{algorithm}% New algorithm
      \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
      \renewcommand{\caption}[2][\relax]{% Make a new \caption
        {\raggedright\textbf{\ALG@name~\thealgorithm} ##2\par}%
        \ifx\relax##1\relax % #1 is \relax
          \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
        \else % #1 is not \relax
          \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
        \fi
        \kern2pt\hrule\kern2pt
      }
  }{% \end{breakablealgorithm}
    \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
    \end{center}
  }
\makeatother

\setcitestyle{comma}
\setlength{\bibsep}{1.5pt}
\setenumerate[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setitemize[1]{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
\setdescription{itemsep=0pt,partopsep=0pt,parsep=\parskip,topsep=5pt}
% 表题 图题 章节号连接符
\renewcommand {\thetable} {\thechapter{}-\arabic{table}}
\renewcommand {\thefigure} {\thechapter{}-\arabic{figure}}
\begin{document}
\captionsetup{labelformat=default, labelsep=space}

% \bibliographystyle{seuthesix}
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}
\newtheorem{definition}{定义}[chapter]
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}  

\categorynumber{TP18} % 分类采用《中国图书资料分类法》
\UDC{004.8}            %《国际十进分类法UDC》的类号
\secretlevel{公开}    %学位论文密级分为"公开"、"内部"、"秘密"和"机密"四种
\studentid{201965 }   %学号要完整，前面的零不能省略。
\title{基于知识图谱表示学习的知识推理方法研究}{}{Research on Knowledge Reasoning Methods based on Knowledge Graph Representation Learning
}{}
\author{周星辰}{Xingchen Zhou}
\advisor{汪鹏}{\  }{Peng Wang}{Associate Prof.}
% 空白的时候需要加转移符
% \advisor{\  }{\  }{ \ }{\  } 
% \coadvisor{楚留香}{副教授}{Perfume Tsu}{Associate Prof.} % 没有% 可以不填
\degreetype{工程硕士}{Master of Engineering} % 详细学位名称
\major{计算机技术}
\submajor{}
\defenddate{}
\authorizedate{\ }
\committeechair{}
\reviewer{}{}
\department{东南大学计算机科学与工程学院}{School of Computer Science and Engineering}
\makebigcover
\makecover

\begin{abstract}{表示学习，知识推理，强化学习，知识图谱}
我是一个人。
\end{abstract}

\begin{englishabstract}{represent learning, knowledge reasoning, reinforcement learning, knowledge graph}
I am a human.
\end{englishabstract} 

\tableofcontents
\mainmatter  % 该命令切换到正文状态。页码从阿拉伯数字 1 开始，此前页码为罗马数字形式。

\chapter{绪论}
\section{研究背景}
知识图谱（Knowledge Graph, KG）最早由Google公司在2012年提出[1]，用于将事实知识进行结构化处理，并将其应用于智能搜索引擎。Google 知识图谱的成功应用，引起了学术界和工业界的广泛关注。知识图谱本质上是结构化的语义知识库，能够解释现实世界中的概念及关系[2]。同时，知识图谱不采用诸如框架和脚本等繁琐的结构，而是采用形式更为灵活简单的<头实体，关系，尾实体>三元组以及与实体和关系相关的属性[3]。其中，实体可以是现实世界的对象和抽象的概念，关系是实体之间的联系，实体和关系具有相应的属性。为了更直观地展示和分析知识图谱，还可以将知识图谱中的实体和关系分别作为节点和边，采用网络图的形式表示知识图谱[3]。随着智能信息化的不断发展，知识图谱已被广泛应用于数据检索[4-6]、智能问答[7,8]、大数据分析决策[9,10]等领域。典型的知识图谱如图1所示，其中包含各种类型的实体和关系信息，同时实体之间存在概念层次结构。

在实践中，知识图谱旨在作为组织或社区内不断发展的知识共享基础。知识图谱可以分为两类，分别是通用知识图谱和领域知识图谱[11]。两类知识图谱的原理相同，区别在于知识范围和应用领域，两种知识图谱在国内外均已得到广泛应用。国外的通用知识图谱包括百科知识图谱Freebase[12]，DBpedia[13]和Yago[14]，多语言百科知识图谱Wikidata[15]，词典知识图谱WordNet[16]和常识知识图谱CYC[17]等。国内的通用知识图谱则包括中文百科知识图谱Zhishi.me[18]，该知识图谱首先采用固定的抽取规则，从百度百科、互动百科和中文维基百科中抽取实体信息，随后对不同百科的实体进行对齐，从而完成实体链接。CN-DBpedia[19]是复旦大学开发的大规模通用领域结构化百科，从中文百科类网站的纯文本信息中提取信息，经过过滤、融合、推断等步骤后构成高质量结构化数据。领域知识图谱主要服务于特定行业领域，随着知识图谱在工业界的推广，逐渐发挥着越来越重要的作用。国外的领域知识图谱包括服务网络搜索的Bing[20]和Google[1]知识图谱，服务社交网络的Facebook[21] 和LinkedIn[21]知识图谱，服务商业领域的Aribnb[22]和Amazon[23]知识图谱，以及服务金融领域的Accenture[24]和Banca d’Italia[25]知识图谱等。国内的领域知识图谱包括百度[26]、淘宝[27]、微博[28]等企业的知识图谱。由于领域知识图谱涉及到具体而复杂的领域场景，需要业务和开发专家的配合，因此对领域知识图谱的设计和开发提出了更高的要求。
知识图谱能够采用简洁的表示形式，将复杂多样的知识转化为清晰的三元组形式，并提供了强大的知识存储和推理能力。然而，知识图谱的知识信息通常采用自然语言进行描述，难以被计算机所处理和应用。因此，为了方便知识图谱的后续应用，需要将知识图谱内的知识信息转换为计算机能够直接处理的向量形式，这项技术被称为知识图谱表示学习（Knowledge Representation Learning, KRL）[29]或知识图谱嵌入（Knowledge Graph Embedding, KGE）[30]。知识图谱表示学习模型能够将知识图谱中的实体和关系映射到低维稠密的向量空间进行表示，同时保留其中的结构和语义信息，从而能够进行知识推理等知识应用工作。知识图谱表示学习主要涉及四个问题：（1）选择何种表示空间：现有的模型采用的表示空间可以分为欧几里得空间[31]、流形空间[32]、复向量空间[32]、高斯分布[33]和离散空间[34]。（2）采用何种评分函数：现有的评分函数包括基于距离的评分函数[35]和基于语义相似度的评分函数[36]。（3）应用何种编码模型：现有模型采用的编码模型包括线性模型[37]、双线性模型[32]、因式分解模型[39]和神经网络模型[40]。（4）是否采用辅助信息，以及采用何种辅助信息：可以加入文本信息、视觉信息和类型信息等多种辅助信息[41]。通过知识图谱表示学习，可以将知识信息转换为向量表示，作为知识补全等下游任务的输入。由于知识图谱通常是采用自动或半自动的方式进行构建，因此通常是不完整的。通过现有知识预测知识图谱中缺失的知识，进行知识图谱补全，是提高知识图谱质量的有效手段[42]。

知识图谱的典型应用中包括知识图谱补全和知识图谱问答。尽管知识图谱表示学习能够完成部分知识图谱补全和知识图谱问答的工作，但无法对关系路径进行建模，而只能基于单个实体或者关系进行简单的补全或问答。同时，知识图谱表示学习通过向量计算的方法进行隐式的知识补全，本质上属于黑盒模型，无法提供充分的证据，可解释性比较差。因此，需要借助知识图谱上的知识推理方法，综合考虑知识图谱网络图的拓扑结构信息，对复杂关系路径进行建模，从而解决更为复杂的知识图谱补全和知识图谱问答内容，并提供相应推理证据[43]。知识推理不局限于传统的基于逻辑的和规则的推理方法，还可以采用更多样化的推理方法，随着知识图谱表示学习等技术的发展，一系列新的知识推理方法不断涌现，例如随机游走方法[44]、路径排序方法[45]、启发式方法[46]和神经网络方法[47]等。丰富的知识图谱内容为知识推理技术的发展提供了新的机遇和挑战。

综上所述，知识图谱在人工智能应用中具有较大价值，基于知识图谱的知识图谱表示学习有助于将知识图谱转化成易于处理和应用的知识图谱表示向量，并为下游任务提供充分的支持。知识推理是知识图谱智能应用的一个重要任务，深入研究该任务，有助于加深对知识图谱的认知，并充分和有效地利用知识图谱的知识。因此，本文旨在提出一种有效的知识图谱表示学习模型，随后基于知识图谱表示学习模型，实现一种知识推理模型，在适用于大规模知识图谱的前提下，同时保证知识推理的质量，并能够解决多种推理任务。
\section{研究现状}
\subsection{知识图谱表示学习研究现状}
知识图谱能够有效地表示知识图谱中的结构化数据，但是难以被计算机进行直接处理和应用，需要先通过知识图谱表示学习的过程将知识三元组转换为向量的形式，才能应用于各类知识图谱推理任务。知识图谱可以表示为，其中E、R和F分别表示实体、关系和事实的集合。知识图谱内的事实三元组可以表示为，其中h、r和t分别表示头实体、关系和尾实体。例如，对于给定的事实三元组，其中的是头实体h，是尾实体t，则是头实体和尾实体之间的关系r。同时根据定义，有，以及。经过知识图谱表示学习后，可以将知识图谱中的节点和关系映射到低维稠密向量空间中。使用粗体字符表示知识图谱表示向量，则完成知识表示后的三元组为，这里设置向量空间为d维的欧几里得空间，也可以投影到其他类型的向量空间。为了构建知识图谱表示学习模型，需要完成三个步骤：（1）定义实体和关系在向量空间的表示形式。在这一步中，需要选择向量空间，设计编码模型，并确定是否需要辅助信息。（2）定义评分函数，用于评价事实的合理性。知识图谱中可见事实三元组的得分一般会高于潜在事实三元组的得分。以及（3）学习实体和关系的表示，使知识图谱中可见事实三元组的总置信度最大化。

目前的知识图谱表示学习模型可以分为翻译模型，语义匹配模型和因果推断模型。

\subsubsection{翻译模型}
翻译模型采用基于距离的评分函数，将关系看做头实体和尾实体之间的翻译，并将关系翻译后两个实体之间的距离作为得分，进而实现衡量事实三元组的合理性。

翻译模型的典型代表是Bordes等人提出的TransE模型[48]，其思想源于Mikolov等人提出的词嵌入模型word2vec[49]，在该模型中发现了词向量空间中存在平移不变性，例如。TransE将知识图谱中的实体和关系映射到d维欧几里得空间中，即，并将关系看做是实体之间的连接向量，遵循，例如。同时对于给定三元组，通过计算和之间的L1或L2距离，进而得到三元组的合理性得分。对于较为合理的三元组，其得分应尽量接近0，对于不合理的三元组，则希望其得分相对较高。TransE模型采用的得分函数可以表示为：。尽管TransE模型在大规模知识图谱表示学习方面取得了巨大进步，但在处理如一对多、多对一和多对多等复杂关系时仍然存在困难。例如，给定两个三元组（北京，所属国家，中国）和（上海，所属国家，中国），其中是多对一类型的关系，根据TransE模型的要求，则实体和会被映射到向量表示空间中十分接近的位置。然而这两个实体实际上存在较大差异，因而这种映射是不合理的。

为了更好地处理复杂关系问题，Wang等人提出的TransH模型[50]扩展了原始的TransE模型，为每种关系分别构建关系超平面，从而让一个实体在不同的关系超平面中会得到不同的表示向量。对于关系r，TransH会采用关系r独有的平移向量和超平面法线向量对关系进行表示。对于给定三元组，TransH首先将h和t的表示向量沿着法线向量的方向映射到关系超平面，设和分别为头实体和尾实体的映射向量，则有。随后用关系平移向量连接头实体向量和尾实体向量。TransH模型的评分函数为。虽然TransH模型使得实体能够根据关系获得不同表示，但全部实体和关系仍然在分布在相同的特征空间中。由于实体具有多种语义，而不同关系可能关注实体的不同语义，因此单个特征空间可能不足以反映实体和关系的这一性质。

为了解决上述问题，Lin等人提出的TransR[51]进一步扩展了TransH，将关系超平面扩展到了关系空间，即为每种关系构建对应的关系空间。对于给定三元组，TransR将实体h和t映射到实体向量空间中，同时为每个关系设置了相应的投影矩阵，能够将实体向量映射到关系r对应的向量空间。设和分别为头实体和尾实体的映射向量，则有。TransR采用投影矩阵，因此时空复杂度高于TransE和TransH。Ji等人提出的TransD[52]模型很好地改善了TransR模型存在的问题。TransD对给定三元组中每个实体和关系都构建两个向量，其中一个向量用来表示实体或关系的含义信息，表示为和，另一个向量用于形成两个动态投影矩阵，表示为和。相应地，构建的动态投影矩阵为，其中是单位矩阵。设和分别为头实体和尾实体的映射向量，则有。通过构建动态投影矩阵，TransD降低了矩阵运算的时空复杂度。

上述模型主要目标在于通过改变实体和关系的表示空间，提高表示学习的效果。除此之外，还有一些研究者从评分函数和编码方式的角度入手改进模型。对于给定的三元组，TransM[53]将与关系相应的权重相关联，并将评分函数定义为。通过给一对多、多对一和多对多关系分配较低的权重，可以让和之间留有更大的距离。MainfoldE[54]将的要求弱化为，从而将t映射到以为中心，半径为的超球体，而不再是靠近的确切点。同时，MainfoldE的评分函数定义为。TransA[55]在TransR的基础上，使用自适应马氏距离代替传统的欧氏距离，并定义评分函数为，从而使模型在处理复杂关系时更加灵活。

\subsubsection{语义匹配模型}
语义匹配模型采用基于相似度的评分函数，通过分析实体和关系的向量空间表示捕获语义信息，进而评估事实的合理性。模型的核心方法是，首先将知识图谱中的事实三元组转换为三维的二元张量，其中n为实体数量，m为关系数量，张量的每个切片对应相应的关系，张量值代表事实三元组是否存在于知识图谱中，如果为1则存在，为0则不存在。

RESCAL[56]是语义匹配模型的典型代表，采用张量来表示知识图谱的结构。对于关系，有张量切片。其中矩阵用于捕获实体的潜在语义表示，矩阵用于对头尾实体对的交互进行建模。对于给定的三元组，RESCAL的评分函数定义为，其中是实体的嵌入向量，表示关系中的潜在语义信息。通过张量分解的方法，RESCAL能够较好地捕获实体和关系的语义信息，并给出三元组的合理性得分。然而，RESCAL的计算复杂度比较高。为了改善模型的时空复杂度，DistMult[57]将限制为对角矩阵，即。随后，评分函数被调整为。通过这样的转换，DistMult显著降低了算法复杂度，同时在实验结果上取得了提高。然而，DistMult对头实体和尾实体的计算是对称的。对此，ComplEx[58]模型通过复数嵌入，能够在不对称关系中扩展DistMult模型。实体和关系被嵌入到了复数空间中，同时评分函数被修改为，其中表示复数值的实数部分，表示尾实体的复数共轭部分。通过该评分函数，具有不对称关系的三元组就可以根据实体的序列获取不同的分数。

另一种具有代表性的语义匹配模型是RotatE[59]。受欧拉恒等式的启发，RotatE引入了旋转哈达玛积，它将关系视为复数空间中头实体到尾实体的旋转，对于给定的三元组，RotatE的评分函数为，其中表示旋转哈达玛积。通过这种方式，RotatE具有较好地表示和推理能力，能够处理对称/反对称关系、自反关系和复合关系。然而，RotatE采用的旋转哈达玛积在处理部分三元组时存在局限性。例如，给定三元组和，RotatE计算时会判断与是逻辑等价的，并将和映射到向量空间中角度接近的位置，但这种推导显然是错误的。为了解决这一问题，QuatE[60]模型将RotatE从复数空间扩展到四维空间，并采用哈密尔顿积替换旋转哈达玛积，用于捕获实体和关系间的潜在语义关联。相应的评分函数为，其中表示哈密尔顿积。由于哈密尔顿积的前后算子不可交换，所以能够较好地解决RotatE存在的问题。

\subsubsection{因果推断模型}
因果推断模型将因果推断方法引入知识图谱表示学习领域，主要思想是引入因果干预方法，增强模型的领域适应性，减少局部结构差异，从而提高模型的泛化能力。同时通过因果分析还能够确定知识图谱表示学习时哪些特征信息有助于进行实体和关系表示，并提高相应的信息权重，进而增强模型的表示效果。

Wang等人提出的NIC模型[61]将因果推断模型与知识图谱表示学习领域相结合，提高了表示效果。知识图谱表示学习模型通过设计表示空间、评分函数和编码模型，并考虑加入辅助信息，完成模型的构建。对于给定的三元组，模型会通过评分函数给出三元组的合理性评分。在训练模型学习实体和关系的向量表示时，通常选择优化目标是使得知识图谱中已有三元组的评分尽可能比未出现的三元组评分更高。然而，训练过程中的评分主要用于进行结果排序，而不是作为三元组的置信度得分。这导致评分函数的效果下降，限制了知识图谱补全等知识推理工作的发展。例如，给定三元组，常规知识图谱表示学习方法的评分函数得到相应评分为0.9，但该三元组的实际置信度得分只有0.3，即评分与置信度得分存在不匹配的问题。因此，为了解决上述问题，使得评分函数能够给出接近实际置信度的评分，NIC提出了一个基于因果干预的置信度评估方法，称为邻里干预一致性（Neighborhood Intervention Consistency，NIC），通过验证预测结果的稳定性来评估置信度分数，具体方法是调整实体向量在不同维度上的值，设置为集合中的任意一种，即可以取零值、平均值、最大值和最小值其中之一。如果调整所有特征中的权值会导致时间效率过低，因此NIC通过特征选择获取不同维度的贡献，贡献计算方法为，并选择贡献最大的维度进行调整。完成调整后，观察模型的输出序列是否改变或者是否与原始序列匹配，匹配度计算方法为，其中和分别为Softmax函数和Sign函数，该函数可以表示因果干预后预测结果的稳健性。随后给出置信度得分为，为0到1之间的实数。

另一种具有代表性的因果推断知识图谱表示学习模型为Feng等人提出的CGI模型[62]，采用图卷积神经网络。图卷积神经网络广泛应用于知识图谱表示学习中，其主要思想是通过聚合实体节点的邻域信息实现实体表示效果的增强。然而，由于知识图谱中局部结构属性存在较大差异，即存在局部结构差异问题，从而造成实体节点表示存在不一致分布的问题，这导致对不同节点使用相同的聚合方法会降低节点表示效果。通常的解决方案是采用注意力机制，通过训练降低可能导致局部结构差异的邻居的权重，然而通常无法实现理想情况的注意力机制效果。针对上述问题，CGI采用了因果图卷积神经网络模型，该模型能够根据局部结构的因果效应调整训练好的图卷积神经网络模型。CGI首先得到节点的原始表示向量，该向量受到邻域特征和自身特征的影响。随后采用因果干预，将邻域置位空，从而实现屏蔽图结构，并要求实体节点根据自身特征获取表示向量。因果干预过程可以表示为，其中表示执行因果干预，将邻域置为空集合。最后CGI根据局部结构的因果效应、预测置信度和其他因素，采用一个独立的二元分类器，在因果干预表示和原始表示之间进行选择。实验证明模型通常能够在遇到局部结构差异时选择因果干预表示，即屏蔽邻域特征仅采用节点自身特征的表示。

\subsection{知识推理研究现状}
通过知识图谱表示学习模型可以通过链接预测的方式实现知识推理，即给定查询三元组，或，通过将知识图谱内所有关系或实体作为候选答案，输入到知识图谱表示学习模型中，从而将实体与关系映射到低维向量空间中，并通过得分函数进行打分，随后根据得分结果进行排序，选择得分最高的实体或关系作为答案，并完成知识推理。然而，基于知识表示学习模型的推理在形式上只能完成单跳的推理，无法显式地进行多跳推理，因而无法提供推理路径。这使得推理过程缺乏可解释性[63]。为了解决这一问题，提出了关系路径推理方法，该类方法利用知识图谱上的拓扑结构信息，在知识图谱的关系路径上进行建模和推理。采用关系路径推理的方式，不仅可以得到推理结果，同时还可以给出一个可解释的路径指示推理过程。

现有的知识推理模型可以分为符号逻辑推理模型，神经推理模型和神经符号推理模型。

\subsubsection{符号推理模型}
符号推理模型（Symbolic reasoning model）旨在为非结构化自然语言查询语句生成结构化的查询。现有的符号推理模型可以分为语义解析和基于模板的知识推理两种方法。其中，语义解析方法通过自然语言处理工具，将问题转换为句法依赖表示。基于模板的知识推理方法则构建大量模板，包括自然语言模式和相应的结构化查询模式（例如SPARQL），从而实现复杂问题的分解。

马尔科夫符号网络（Markov Logic Network, MLN）[64]是基于预定义的规则和知识图谱中的事实三元组信息构建的概率图模型，能够在模型训练的过程中学习不同规则的权重，最终。具体而言，给定一个具体的规则集合，其中的每个规则都可以通过知识三元组进行具体查询，则可以通过下列步骤完成MLN模型的构建：首先为每个规则对应的知识三元组构建一个对应节点，如果知识三元组存在则节点值设为1，否则设为0。随后如果两个节点如果对应同一个规则，则在两个节点之间构建一条边。最后令每个规则对应一个特征，如果规则成立则特征值为1，否则特征值为0。规则的所有节点都共享这个特征。完成MLN模型构建后，网络中所有节点的联合分布定义为，其中是对应规则的权重，是规则对应节点的数量。随后可以通过MCMC算法进行MLN模型的推理，并通过优化伪似然度量实现权重的学习。基于学习的权重，即可以完成具体的路径推理。通过引入权重信息，MLN能够较好地将一阶谓词逻辑和概率图模型进行结合，并实现不确定性的推理。然而，知识图谱结构十分复杂，造成了MLN的推理非常困难且效率低下。此外，知识图谱中缺失的三元组也会影响规则推理的效果。为了解决上述问题，pLogicNet[65]综合了MLN和图嵌入技术的思想，首先通过MLN定义知识图谱中事实三元组的联合分布，并对每种逻辑规则与权重构建关联，随后采用变分EM算法有效地学习权重。在EM算法中，E步骤推断未观察到的三元组的合理性，其对应的变分分布采用TransE等知识图谱表示学习模型进行参数化，M步通过优化观察到的三元组和知识图谱表示学习模型得到的三元组上的伪似然来更新逻辑规则的权重。通过上述方法，pLogicNet可以实现高效知识推理，同时能够处理知识图谱中缺失的信息。

另一类符号逻辑推理模型的代表是ProbLog[66]，它是逻辑编程语言ProLog的一个改进。ProLog能够基于给定规则、事实和查询语句进行自动的知识推理并给出答案。ProbLog则是对于每种规则设置了一个概率，并令最终的知识推理给出推理的概率，从而模拟知识推理过程中的不确定性。给定查询，则推理概率被定义为，其中T描述了原因L的概率分布，因此推理概率被分解为查询的所有联合概率和每种可能的原因集合的总和。其中有 ，进一步分解了推理概率，其中表示给定原因的情况下查询的概率，如果至少有一个答案能够使查询为真，则对应的的值为1。式中另一项，即原因集合的概率的计算方法为。为了计算，一种方法是枚举所有可能的逻辑规则和对应的事实，效率较低。另一种方法是采用ProLog的选择线性确定（Selective Linear Definite，SLD）方案，采用自上而下的方式构造SLD树。首先通过查询步骤初始化根节点，随后通过应用每个规则及对应的事实来递归创建子目标，到达结束条件时停止迭代，即找到了查询目标或到达了树的最大深度。同时，每种查询结果都与一系列带有概率的规则相关联。

\subsubsection{神经推理模型}
神经推理模型的推理方法是将知识图谱中的实体、关系和查询问题共同编码到相同的表示空间中，并在表示空间中进行知识推理。传统的基于距离和基于语义匹配的模型无法满足知识推理的要求，为了获得更加有效的实体和关系嵌入效果，神经推理模型引入了神经网络结构，用于知识图谱邻域信息的建模。神经推理模型与知识图谱表示学习模型比较相似，都是首先进行知识图谱表示，随后实现知识推理等任务，但不同在于，神经推理模型会将自然语言查询问题也编码到表示空间中，从而增强知识推理的效果。同时，神经推理模型能够处理单跳问答、多跳问答和复杂逻辑问答问题，而知识图谱表示学习模型通常只能处理单跳问答问题。

ConvE[67]是第一种采用卷积神经网络的神经推理模型，首先将头实体和关系映射到向量空间，随后将向量转换成矩阵并进行拼接，输入到卷积层中。ConvE采用二维卷积捕获两个实体之间的特征交互信息，并通过卷积层和全连接层获取实体之间不同维度的局部信息，返回特征图张量。随后，将张量转化为向量，并投影到向量空间，最后通过计算与尾实体向量的内积，得到最终的合理性得分。其评分函数为，其中为卷积核，为权重矩阵。此外，ConvE还引入了一个一对多的评分程序，能够自动匹配所有尾实体，从而能够更有效率地评估所有候选答案尾实体。然而，ConvE将实体和关系向量转换为矩阵并进行拼接，这一过程丢失了翻译模型的平移不变性。对此，ConvKB[68]去除了ConvE的转换操作，并采用一维卷积保持平移不变性，同时捕获实体之间的全局关系和平移特征。ConvKB将每个三元组的d维度嵌入表示为一个三列的矩阵，随后将矩阵输入到卷积层，其中提供许多相同维度的过滤器，能够提取三元组中相同维度特征之间的全局关系，并通过对矩阵的每行进行操作以获得不同的特征图，最后将这些特征图拼接成一个三元组特征向量，并通过和权重矩阵进行点积运算得到最终的三元组合理性得分，评分函数为，其中g为激活函数，为过滤器，为权重矩阵。HypER[69]是基于ConvE提出的又一种卷积神经网络推理模型，采用基于ConvE的超网络为每个关系生成卷积滤波器权重，超网络可以实现跨层的权重共享，并动态生成给定输入的权重。HypER使用基于特定关系的一维卷积滤波器来处理实体嵌入，这简化了嵌入模式。同时，HypER使用卷积算子实现头实体嵌入和一组特定于关系的滤波器，滤波器是超网络H从关系嵌入中创建的，超网络的结构是一个全连接层。最后，HypER还应用了ReLU激活函数的权重矩阵实现特征图张量的向量化。HypER的评分函数为，相较于ConvE，HypER具有更强的表达能力和更少的参数。

传统的神经推理模型通过单向关系路径或文本信息引入上下文信息，而忽略了实体之间的各种连接模式，以及不重要的路径和词的影响，这造成模型无法充分捕捉三元组之间的关系信息。为了解决这一问题，R-GCN[70]采用关系图卷积网络，显式地对多关系知识图谱进行建模并捕获实体周围的单跳邻域事实信息。R-GCN由编码器和解码器构成，在对实体进行编码的时候，R-GCN能够综合考虑实体连接之间的各类输入和输出的关系信息，并对实体的结构信息进行充分的编码。实体表示被更新为，其中是节点在关系下的邻域，是特定问题的归一化常数。解码时，可以通过基数分解和块对角矩阵分解的方法求解，并选择DistMult分解模型作为评分函数，认为每个关系都对应一个对角矩阵，从而得到最终的评分函数为。R-GCN分析了知识图谱中邻域信息对实体表示的影响，但是没有显式地对实体间的关系进行建模。SACN[71]模型对R-GCN进行了改进，是一种加权的GCN模型，能够同时考虑节点间的连通性、节点的属性和关系类型。SACN同样由编码器和解码器构成。编码器部分，SCAN引入了不同类型关系的权重以对GCN模型进行改进，得到加权图神经网络。同时，在聚合信息时对不同类型的关系分别进行聚合，因此可以将多关系图按照多个单关系图的形式进行分析。此外，SACN还将实体属性作为属性节点加入到了实体表示部分，并采用类似的方法进行分析。在SACN模型中，实体表示被更新为。解码器采用Conv-TransE进行解码，利用卷积过滤器直接对相同维度的实体和关系向量进行处理。

\subsubsection{神经符号推理模型}
神经符号推理模型（Neural-symbolic reasoning model）是符号推理模型和神经推理模型的结合，综合了两种模型的优点。现有的神经符号推理模型包括神经增强符号推理模型（Neural-enhanced symbolic reasoning model）和端到端推理模型。其中，神经增强符号推理模型仅仅用于将问题解析成结构化的查询，随后需要使用查询方法才能够得到最终答案。而端到端推理模型可以自动完成解析问题并检索答案的步骤。

DeepPath[63]首先将强化学习模型应用到了关系路径知识推理方面，并开发了一种新的奖励函数，通过综合分析模型是否到达目标节点、模型推理路径长度是否过长和与已有路径嵌入向量的相似度，来提高准确性、路径多样性和推理效率。DeepPath首先采用翻译模型的方法，对连续空间中的代理状态信息进行编码，并选择知识图谱的关系空间作为强化学习模型的动作空间。另一种基于强化学习的模型是MINERVA[72]，根据基于LSTM的策略函数对邻域信息进行采样。同时，与DeepPath不同的是，MINERVA中的状态是由查询关系和部分路径的嵌入组成，在采样的过程中不需要答案实体的嵌入向量，从而能够允许MINERVA直接通过知识推理得到答案。同时，MINERVA还为知识图谱的每个节点增加了自关系，从而能够在推理过程中在到达答案实体时自动停止。然而DeepPath和MINERVA在分析模型是否到达目标节点时，提供的奖励机制比较简单，即如果代理到达目标节点则提供值为1的奖励，同时对没有达到目标节点的情况提供值为-1的惩罚或者值为0的奖励。这种方法导致了奖励稀疏问题并降低了模型探索路径的效果。对此，Multi-Hop模型[73]改进了强化学习的奖励函数，将硬奖励替换成了基于真实答案实体和推理实体之间相似度的软奖励。同时，受到dropout技术的启发，Multi-Hop在模型训练的过程中丢弃了一些动作，从而避免选择大量重复路径，缓解过拟合问题。另一种缓解奖励稀疏的方法由M-Walk[74]模型提出，该模型采用了一种基于价值的强化学习方法，并使用蒙特卡洛树搜索（Monte Carlo Tree Search，MCTS）来客服奖励稀疏的问题。具体而言，M-Walk迭代地使用MCTS的轨迹生成步骤和策略改进步骤，实现细化策略函数，从而获得更多的奖励信息。

除去关系路径知识推理模型，另一种神经符号推理模型采用图结构进行推理。GraIL模型[75]采用图神经网络的形式，并在提取的子图上推理两个实体之间的关系。具体而言，GraIL首先提取头实体和尾实体周围的跳子图并取交集，随后采用元组标记子图中的实体，其中表示子图中的第个节点，表示头实体和尾实体到节点的最短距离。随后采用基于注意力的多关系图神经网络，也就是R-GCN，计算每个节点的实体表示，最后将头实体、尾实体和查询关系的表示向量连接起来，并以此为基础在子图上进行推理。GraIL在学习与实体无关的关系语义方面具有很强的能力。CogGraph[76]从给定的头实体和查询关系开始，通过策略函数在每一步扩展多个实体，随后采用GNN模型获取节点在推理子图中的表示向量，并预测答案。DPMPN模型[77]采用了两个图神经网络模型来执行推理。第一个模型在整个知识图谱上进行消息传递，用于提供实体表示。第二个模型在查询子图上修建消息传递图，用于捕获与查询相关的语义。

此外，还有基于矩阵的神经符号推理模型。这种模型可以看做图神经网络模型的扩展，其不会基于跳数选择邻居，而是综合软注意力分数来评估不同邻居的重要性。其基本思想是通过矩阵运算来表达头尾实体之间的重要性。TensorLog[78]是较早提出的基于矩阵的模型，能够通过知识推理，得到带有权重的链式逻辑规则，从而解释知识图谱中的每个关系。随后基于推理得到的规则，输入头实体和关系，目标是通过检索对实体进行排名，将正确答案尽可能排在靠前的位置。TensorLog采用独热向量表示实体，并用矩阵表示知识图谱中的向量，其中如果知识存在于知识图谱中，则矩阵对应位置为1。但是，TensorLog中的参数较难学习，因为每种规则都对应一个参数，而枚举规则本身就是一个离散的任务。对此，神经逻辑编程（Neural LP）模型[79]将规则的权重参数分解为了规则中谓词的权重。同时，设计了一个循环公式来动态建模规则的长度。最后，Neural LP计算所有向量的加权平均值，并使用注意力来控制规则的长度。上述两种方法可以学习一些概率链式逻辑规则，但是无法推断出一些复杂的规则形式，如树状规则。此外，链式规则是基于特定的头实体进行推断的，这也会降低学习的泛化性。对此，神经逻辑归纳学习（Neural Logic Inductive Learning, NLIL）模型[80]将原始语句进行合并以处理非链式规则，并取得较好的效果。

\subsection{存在问题分析}

\section{论文工作}

\section{论文组织结构}

\chapter{基于因果推断的知识图谱表示学习}
\section{知识图谱表示学习任务定义}

\section{基于因果推断的知识图谱表示学习模型框架概览}

\section{层次信息提取模块构建}

\section{知识图谱嵌入模块构建}

\section{因果推断模块构建}

\section{本章小结}

\chapter{基于双重代理强化学习的知识推理}
\section{知识推理任务定义}

\section{基于双重代理强化学习的知识推理模型框架概览}

\section{强化学习模块构建}

\section{双重代理交互模块构建}

\section{本章小结}

\chapter{实验评估}
\section{实验环境}

\section{知识图谱表示学习任务实验与评估}
\subsection{评价指标}

\subsection{数据集}

\subsection{实验设置}

\subsection{实验结果与分析}

\section{知识推理任务实验与评估}
\subsection{评价指标}

\subsection{数据集}

\subsection{实验设置}

\subsection{实验结果与分析}

\section{本章小结}

\chapter{总结与展望}
\section{主要工作总结}

\section{未来工作展望}



\acknowledgement

% \bibliographystyle{seuthesix}
\thesisbib{references}  % 该命令用于生成参考文献，采用 BIBTEX 工具自动生成。为此，需提供.bib 数据库文件名称作为该命令的参数，不需要包含扩展名。
\bibliographystyle{seuthesix}
% \bibliographystyle{unsrt}
\bibliography{paper}

\resume{作者简介}

\end{document}
